@inproceedings{sivathanu_astra:_2019,
	location = {Providence, {RI}, {USA}},
	title = {Astra: Exploiting Predictability to Optimize Deep Learning},
	isbn = {978-1-4503-6240-5},
	url = {http://dl.acm.org/citation.cfm?doid=3297858.3304072},
	doi = {10.1145/3297858.3304072},
	shorttitle = {Astra},
	abstract = {We present Astra, a compilation and execution framework that optimizes execution of a deep learning training job. Instead of treating the computation as a generic data flow graph, Astra exploits domain knowledge about deep learning to adopt a custom approach to compiler optimization.},
	eventtitle = {the Twenty-Fourth International Conference},
	pages = {909--923},
	booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems - {ASPLOS} '19},
	publisher = {{ACM} Press},
	author = {Sivathanu, Muthian and Chugh, Tapan and Singapuram, Sanjay S. and Zhou, Lidong},
	urldate = {2019-11-24},
	date = {2019},
	langid = {english},
	file = {Sivathanu et al. - 2019 - Astra Exploiting Predictability to Optimize Deep .pdf:files/5/Sivathanu et al. - 2019 - Astra Exploiting Predictability to Optimize Deep .pdf:application/pdf}
}

@inproceedings{hua_boosting_2019,
	location = {Columbus, {OH}, {USA}},
	title = {Boosting the Performance of {CNN} Accelerators with Dynamic Fine-Grained Channel Gating},
	isbn = {978-1-4503-6938-1},
	url = {http://dl.acm.org/citation.cfm?doid=3352460.3358283},
	doi = {10.1145/3352460.3358283},
	abstract = {This paper proposes a new fine-grained dynamic pruning technique for {CNN} inference, named channel gating, and presents an accelerator architecture that can effectively exploit the dynamic sparsity. Intuitively, channel gating identifies the regions in the feature map of each {CNN} layer that contribute less to the classification result and turns off a subset of channels for computing the activations in these less important regions. Unlike static network pruning, which removes redundant weights or neurons prior to inference, channel gating exploits dynamic sparsity specific to each input at run time and in a structured manner. To maximize compute savings while minimizing accuracy loss, channel gating learns the gating thresholds together with weights automatically through training. Experimental results show that the proposed approach can significantly speed up state-of-the-art networks with a marginal accuracy loss, and enable a trade-off between performance and accuracy. This paper also shows that channel gating can be supported with a small set of extensions to a {CNN} accelerator, and implements a prototype for quantized {ResNet}-18 models. The accelerator shows an average speedup of 2.3× for {ImageNet} when the theoretical {FLOP} reduction is 2.8×, indicating that the hardware can effectively exploit the dynamic sparsity exposed by channel gating.},
	eventtitle = {the 52nd Annual {IEEE}/{ACM} International Symposium},
	pages = {139--150},
	booktitle = {Proceedings of the 52nd Annual {IEEE}/{ACM} International Symposium on Microarchitecture  - {MICRO} '52},
	publisher = {{ACM} Press},
	author = {Hua, Weizhe and Zhou, Yuan and De Sa, Christopher and Zhang, Zhiru and Suh, G. Edward},
	urldate = {2019-11-24},
	date = {2019},
	langid = {english},
	file = {Hua et al. - 2019 - Boosting the Performance of CNN Accelerators with .pdf:files/7/Hua et al. - 2019 - Boosting the Performance of CNN Accelerators with .pdf:application/pdf}
}

@inproceedings{deng_tie:_2019,
	location = {Phoenix, Arizona},
	title = {{TIE}: energy-efficient tensor train-based inference engine for deep neural network},
	isbn = {978-1-4503-6669-4},
	url = {http://dl.acm.org/citation.cfm?doid=3307650.3322258},
	doi = {10.1145/3307650.3322258},
	shorttitle = {{TIE}},
	abstract = {In the era of artificial intelligence ({AI}), deep neural networks ({DNNs}) have emerged as the most important and powerful {AI} technique. However, large {DNN} models are both storage and computation intensive, posing significant challenges for adopting {DNNs} in resourceconstrained scenarios. Thus, model compression becomes a crucial technique to ensure wide deployment of {DNNs}. This paper advances the state-of-the-art by considering tensor train ({TT}) decomposition, an very promising but yet explored compression technique in architecture domain. The method features with the extremely high compression ratio. However, the challenge is that the inference on the {TT}-format {DNN} models inherently incurs massive amount of redundant computations, causing significant energy consumption. Thus, the straightforward application of {TT} decomposition is not feasible.},
	eventtitle = {the 46th International Symposium},
	pages = {264--278},
	booktitle = {Proceedings of the 46th International Symposium on Computer Architecture  - {ISCA} '19},
	publisher = {{ACM} Press},
	author = {Deng, Chunhua and Sun, Fangxuan and Qian, Xuehai and Lin, Jun and Wang, Zhongfeng and Yuan, Bo},
	urldate = {2019-11-24},
	date = {2019},
	langid = {english},
	file = {Deng et al. - 2019 - TIE energy-efficient tensor train-based inference.pdf:files/9/Deng et al. - 2019 - TIE energy-efficient tensor train-based inference.pdf:application/pdf}
}

@inproceedings{besta_slim_2019,
	location = {Denver, Colorado},
	title = {Slim graph: practical lossy graph compression for approximate graph processing, storage, and analytics},
	isbn = {978-1-4503-6229-0},
	url = {http://dl.acm.org/citation.cfm?doid=3295500.3356182},
	doi = {10.1145/3295500.3356182},
	shorttitle = {Slim graph},
	abstract = {We propose Slim Graph: the first programming model and framework for practical lossy graph compression that facilitates high-performance approximate graph processing, storage, and analytics. Slim Graph enables the developer to express numerous compression schemes using small and programmable compression kernels that can access and modify local parts of input graphs. Such kernels are executed in parallel by the underlying engine, isolating developers from complexities of parallel programming. Our kernels implement novel graph compression schemes that preserve numerous graph properties, for example connected components, minimum spanning trees, or graph spectra. Finally, Slim Graph uses statistical divergences and other metrics to analyze the accuracy of lossy graph compression. We illustrate both theoretically and empirically that Slim Graph accelerates numerous graph algorithms, reduces storage used by graph datasets, and ensures high accuracy of results. Slim Graph may become the common ground for developing, executing, and analyzing emerging lossy graph compression schemes.},
	eventtitle = {the International Conference for High Performance Computing, Networking, Storage and Analysis},
	pages = {1--25},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis on   - {SC} '19},
	publisher = {{ACM} Press},
	author = {Besta, Maciej and Weber, Simon and Gianinazzi, Lukas and Gerstenberger, Robert and Ivanov, Andrey and Oltchik, Yishai and Hoefler, Torsten},
	urldate = {2019-11-24},
	date = {2019},
	langid = {english},
	file = {Besta et al. - 2019 - Slim graph practical lossy graph compression for .pdf:files/11/Besta et al. - 2019 - Slim graph practical lossy graph compression for .pdf:application/pdf}
}

@inproceedings{dhulipala_low-latency_2019,
	location = {Phoenix, {AZ}, {USA}},
	title = {Low-latency graph streaming using compressed purely-functional trees},
	isbn = {978-1-4503-6712-7},
	url = {http://dl.acm.org/citation.cfm?doid=3314221.3314598},
	doi = {10.1145/3314221.3314598},
	abstract = {There has been a growing interest in the graph-streaming setting where a continuous stream of graph updates is mixed with graph queries. In principle, purely-functional trees are an ideal fit for this setting as they enable safe parallelism, lightweight snapshots, and strict serializability for queries. However, directly using them for graph processing leads to significant space overhead and poor cache locality. This paper presents C-trees, a compressed purely-functional search tree data structure that significantly improves on the space usage and locality of purely-functional trees. We design theoretically-efficient and practical algorithms for performing batch updates to C-trees, and also show that we can store massive dynamic real-world graphs using only a few bytes per edge, thereby achieving space usage close to that of the best static graph processing frameworks.},
	eventtitle = {the 40th {ACM} {SIGPLAN} Conference},
	pages = {918--934},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation  - {PLDI} 2019},
	publisher = {{ACM} Press},
	author = {Dhulipala, Laxman and Blelloch, Guy E. and Shun, Julian},
	urldate = {2019-11-24},
	date = {2019},
	langid = {english},
	file = {Dhulipala et al. - 2019 - Low-latency graph streaming using compressed purel.pdf:files/13/Dhulipala et al. - 2019 - Low-latency graph streaming using compressed purel.pdf:application/pdf}
}

@inproceedings{dong_network_2019,
	location = {Anchorage, {AK}, {USA}},
	title = {Network Density of States},
	isbn = {978-1-4503-6201-6},
	url = {http://dl.acm.org/citation.cfm?doid=3292500.3330891},
	doi = {10.1145/3292500.3330891},
	abstract = {Spectral analysis connects graph structure to the eigenvalues and eigenvectors of associated matrices. Much of spectral graph theory descends directly from spectral geometry, the study of differentiable manifolds through the spectra of associated differential operators. But the translation from spectral geometry to spectral graph theory has largely focused on results involving only a few extreme eigenvalues and their associated eigenvalues. Unlike in geometry, the study of graphs through the overall distribution of eigenvalues — the spectral density — is largely limited to simple random graph models. The interior of the spectrum of real-world graphs remains largely unexplored, difficult to compute and to interpret.},
	eventtitle = {the 25th {ACM} {SIGKDD} International Conference},
	pages = {1152--1161},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining  - {KDD} '19},
	publisher = {{ACM} Press},
	author = {Dong, Kun and Benson, Austin R. and Bindel, David},
	urldate = {2019-11-24},
	date = {2019},
	langid = {english},
	file = {Dong et al. - 2019 - Network Density of States.pdf:files/15/Dong et al. - 2019 - Network Density of States.pdf:application/pdf}
}

@article{frankle_lottery_2019,
	title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
	url = {http://arxiv.org/abs/1803.03635},
	shorttitle = {The Lottery Ticket Hypothesis},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difﬁcult to train from the start, which would similarly improve training performance.},
	journaltitle = {{arXiv}:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	urldate = {2019-11-24},
	date = {2019-03-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annotation = {Comment: {ICLR} camera ready},
	file = {Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:files/17/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf}
}

@article{ma_neugraph:_nodate,
	title = {{NeuGraph}: Parallel Deep Neural Network Computation on Large Graphs},
	abstract = {Recent deep learning models have moved beyond low dimensional regular grids such as image, video, and speech, to highdimensional graph-structured data, such as social networks, ecommerce user-item graphs, and knowledge graphs. This evolution has led to large graph-based neural network models that go beyond what existing deep learning frameworks or graph computing systems are designed for. We present {NeuGraph}, a new framework that bridges the graph and dataﬂow models to support efﬁcient and scalable parallel neural network computation on graphs. {NeuGraph} introduces graph computation optimizations into the management of data partitioning, scheduling, and parallelism in dataﬂow-based deep learning frameworks. Our evaluation shows that, on small graphs that can ﬁt in a single {GPU}, {NeuGraph} outperforms state-of-theart implementations by a signiﬁcant margin, while scaling to large real-world graphs that none of the existing frameworks can handle directly with {GPUs}.},
	pages = {16},
	author = {Ma, Lingxiao and Yang, Zhi and Wu, Ming and Miao, Youshan and Zhou, Lidong and Xue, Jilong and Dai, Yafei},
	langid = {english},
	file = {Ma et al. - NeuGraph Parallel Deep Neural Network Computation.pdf:files/20/Ma et al. - NeuGraph Parallel Deep Neural Network Computation.pdf:application/pdf}
}